{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "\n",
    "    # mount google drive and move to directory\n",
    "    #from google.colab import drive\n",
    "    #drive.mount('/content/drive')\n",
    "    #%cd 'drive/MyDrive/landscape_cmr'\n",
    "\n",
    "    # install local lbrary and nbdev\n",
    "    !pip3 install -e . -q\n",
    "    !pip3 install nbdev -q\n",
    "\n",
    "    # notebook specific libraries\n",
    "    !pip install -U sentence-transformers\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Revised Landscape Model\n",
    "\n",
    "Here we reproduce the revision of the Landscape model of reading comprehension presented by Yeari and van den Broek (2016). The model integrates the dynamic landscape model of reading comprehension originally characterized by van den Broek (1996) with a latent semantic analysis (LSA) representation of semantic knowledge. This revised landscape model (LS-R model) computes fluctuations in the activation of text units and the interconnections established between them throughout reading. Our implemention of the landscape model is, however, agnostic about the basis of representations of semantic knowledge.\n",
    "\n",
    "> Yeari, M., & van den Broek, P. (2016). A computational modeling of semantic knowledge in reading comprehension: Integrating the landscape model with latent semantic analysis. Behavior research methods, 48(3), 880-896.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np # for loading data\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "password=input()\n",
    "!git clone https://spectraldoy:{password}@github.com/vucml/landscape_cmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Text Similarity\n",
    "This should be as part of the data preprocessing, data can be saved as dictionaries with:\n",
    "```python\n",
    "{\n",
    "  \"text_units\": torch.Tensor of text unit indices,\n",
    "  \"init_connections\": initial TextSimilarity between text units\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarity(nn.Module):\n",
    "  def __init__(self, model_name=\"stsb-distilbert-base\"):\n",
    "    super(TextSimilarity, self).__init__()\n",
    "    self.model_name = model_name\n",
    "\n",
    "    # get the model and tokenizer, assuming we are using DistilBert\n",
    "    self.model = SentenceTransformer(model_name)\n",
    "    self.to(device)\n",
    "  \n",
    "  @staticmethod\n",
    "  def cosine_similarity(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Custom implementation of cosine similarities\n",
    "    \"\"\"\n",
    "    norm = x.norm(dim=-1).unsqueeze(0)\n",
    "\n",
    "    # this is the formula for cosine similarities in a symmetric matrix\n",
    "    return x @ x.t() / (norm.t() @ norm)\n",
    "\n",
    "  def forward(self, cycles: list) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Assumes input is a list of lists of sentences/text units\n",
    "    i.e. a List of Lists of strings\n",
    "    \"\"\"\n",
    "    embeddings = torch.cat([self.model.encode(i, convert_to_tensor=True) for i in cycles])\n",
    "    init_connections = self.cosine_similarity(embeddings)\n",
    "    return init_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3610a286ca48493498e693bf5a46d54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244715968.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ts = TextSimilarity(\"stsb-distilbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2982, 0.8228, 0.6095, 0.6743, 0.2394, 0.5123, 0.2622],\n",
       "        [0.2982, 1.0000, 0.4641, 0.3621, 0.4675, 0.3461, 0.4138, 0.2596],\n",
       "        [0.8228, 0.4641, 1.0000, 0.6026, 0.5938, 0.2347, 0.4832, 0.2477],\n",
       "        [0.6095, 0.3621, 0.6026, 1.0000, 0.5718, 0.3813, 0.4642, 0.2380],\n",
       "        [0.6743, 0.4675, 0.5938, 0.5718, 1.0000, 0.4362, 0.6411, 0.3194],\n",
       "        [0.2394, 0.3461, 0.2347, 0.3813, 0.4362, 1.0000, 0.4827, 0.3655],\n",
       "        [0.5123, 0.4138, 0.4832, 0.4642, 0.6411, 0.4827, 1.0000, 0.3622],\n",
       "        [0.2622, 0.2596, 0.2477, 0.2380, 0.3194, 0.3655, 0.3622, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reading_cycles = [\n",
    "  [\"this is one\", \"of the many\", \"we have\", \"to use\"],\n",
    "  [\"this is another\"],\n",
    "  [\"do you think\", \"this could be\", \"a final?\"]\n",
    "]\n",
    "ts(reading_cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandscapeRevised(nn.Module):\n",
    "    \"\"\"\n",
    "    The landscape model of reading as revised by Yeari and van den Broek (1996).\n",
    "    To encode a text into the model, it is initially segmented into text units \n",
    "    (e.g., words or propositions) and reading cycles (e.g., clauses or  \n",
    "    sentences) depending on researcher preference. Similarly, semantic \n",
    "    connections between all text units are also computed before model \n",
    "    initialization. In the original specification of LS-R, semantic \n",
    "    connections are computed using LSA, but we leave configuration of initial \n",
    "    semantic connections separate from the model. This revised landscape model \n",
    "    computes fluctuations in the _activation_ of text units and in the \n",
    "    _interconnections_ established between them throughout reading.\n",
    "    \n",
    "    Attributes:\n",
    "    - activations: vector, current activation of each relevant text unit\n",
    "    - connections: array, current connection strengths between text units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, connections, \n",
    "                 stop_probability_scale,\n",
    "                 stop_probability_growth,\n",
    "                 choice_sensitivity,\n",
    "                 max_activity=1.0, \n",
    "                 min_activity=0.0, \n",
    "                 decay_rate=0.1, \n",
    "                 memory_capacity=5.0, \n",
    "                 learning_rate=0.9, \n",
    "                 semantic_strength=1.0):\n",
    "        \"\"\"\n",
    "        Initializes model instance with the specified parameter configuration.\n",
    "        Every relevant text unit comes with an activation level and set of \n",
    "        connection weights to every other text unit. Activation levels are \n",
    "        initialized to 0, while initial connection weights are specified by \n",
    "        the connections parameter. Other parameters regulate fluctuations in \n",
    "        unit activations and connection weights throughout reading.\n",
    "        Parameters:  \n",
    "        - connections: array, initial connection strengths between text units  \n",
    "        - max_activity: maximum activation that units are allowed to have  \n",
    "        - min_activity: minimum activation that units are allowed to have  \n",
    "        - decay_rate: decay rate of unit activation from one cycle to next  \n",
    "        - memory_capacity: total activation possible in any given cycle  \n",
    "        - learning_rate: rate of connection weight changes across cycles  \n",
    "        - semantic_strength: relative contribution of initial semantic \n",
    "            connections to computation of overall connection strengths  \n",
    "        \"\"\"\n",
    "        super(LandscapeRevised, self).__init__()\n",
    "\n",
    "        # set initial parameters\n",
    "        self.max_activity = nn.Parameter(\n",
    "            torch.tensor([max_activity], dtype=torch.float32, device=device)\n",
    "        )\n",
    "        self.min_activity = nn.Parameter(\n",
    "            torch.tensor([min_activity], dtype=torch.float32, device=device)\n",
    "        )\n",
    "        self.decay_rate = nn.Parameter(\n",
    "            torch.tensor([decay_rate], dtype=torch.float32, device=device)\n",
    "        )\n",
    "        self.memory_capacity = nn.Parameter(\n",
    "            torch.tensor([memory_capacity], dtype=torch.float32, device=device)\n",
    "        )\n",
    "        self.learning_rate = nn.Parameter(\n",
    "            torch.tensor([learning_rate], dtype=torch.float32, device=device)\n",
    "        )\n",
    "        self.semantic_strength = nn.Parameter(\n",
    "            torch.tensor([semantic_strength], dtype=torch.float32, device=device)\n",
    "        )\n",
    "\n",
    "        # model architecture is set of activations and connections across units\n",
    "        # diagonal of connections is nan since we disallow self-connections\n",
    "        self.unit_count = len(connections)\n",
    "        self.connections = connections * self.semantic_strength\n",
    "        self.activations = torch.zeros(self.unit_count, device=device) + self.min_activity\n",
    "\n",
    "        # retrieval parameters\n",
    "        self.choice_sensitivity = choice_sensitivity\n",
    "        self.stop_probability_scale = stop_probability_scale\n",
    "        self.stop_probability_growth = stop_probability_growth\n",
    "\n",
    "        # other variables to help track encoding and retrieval across trials\n",
    "        self.recall_total = 0\n",
    "        self.encoding_index = 0\n",
    "        self.retrieving = False\n",
    "        self.recall = torch.zeros(self.unit_count, device=device)\n",
    "        self.preretrieval_activations = self.activations.clone()\n",
    "\n",
    "    def reset(self, new_connections):\n",
    "        \"\"\"\n",
    "        Reinitializes model state with the specified connections, but does not\n",
    "        change the parameters, so that parameter learning can continue.\n",
    "        \n",
    "        Every relevant text unit comes with an activation level and set of \n",
    "        connection weights to every other text unit. Activation levels are \n",
    "        initialized to 0, while initial connection weights are specified by \n",
    "        the connections parameter. Other parameters regulate fluctuations in \n",
    "        unit activations and connection weights throughout reading.\n",
    "        Parameters:  \n",
    "        - new_connections: array, new initial connection strengths between text units\n",
    "        \"\"\"\n",
    "        # model architecture is set of activations and connections across units\n",
    "        # diagonal of connections is nan since we disallow self-connections\n",
    "        self.unit_count = new_connections.shape[-1]\n",
    "        self.connections = new_connections * self.semantic_strength\n",
    "        self.activations = torch.zeros(self.unit_count, device=device) + self.min_activity\n",
    "\n",
    "        # other variables to help track encoding and retrieval across trials\n",
    "        self.recall_total = 0\n",
    "        self.encoding_index = 0\n",
    "        self.retrieving = False\n",
    "        self.recall = torch.zeros(self.unit_count, device=device)\n",
    "        self.preretrieval_activations = self.activations.clone()\n",
    "\n",
    "    def experience(self, cycles):\n",
    "        \"\"\"\n",
    "        Updates activations and connections based on content of current \n",
    "        reading cycle.\n",
    "        Activations are updated as a function of three simulated mechanisms:  \n",
    "        1. attention: units of the current cycle are activated to the \n",
    "            highest value  \n",
    "        2. working memory: units from prior cycles carry residual activation \n",
    "            (following a decay rule)  \n",
    "        3. long-term memory: units from prior cycles are reactivated via\n",
    "            connections with text units that are active in the current cycle.  \n",
    "        Episodic connections are added to and augment baseline connection \n",
    "        strengths throughout the dynamic flow from one reading cycle to the \n",
    "        next. They are formed between text units that are coactivated (due to \n",
    "        any activation mechanism) in the same reading cycle. The strength of \n",
    "        episodic connections is a function of the activation levels of the \n",
    "        interconnected text units, and it accumulates with each concurrent \n",
    "        activation (following a logarithmic learning rule).  \n",
    "        Argument:  \n",
    "        - cycles: vector of counts of units processed in each cycle\n",
    "        \"\"\"\n",
    "\n",
    "        for cycle in cycles:\n",
    "            # correct cycle, assuming iterable\n",
    "            for i in cycle:\n",
    "              if i < 0 or i >= self.unit_count:\n",
    "                raise IndexError(f\"Index {i} in cycle is out of range\")\n",
    "\n",
    "            # experience\n",
    "            self.update_activations(cycle) # removed += encoding index\n",
    "            self.update_connections(self.activations)\n",
    "            self.encoding_index += len(cycle)\n",
    "        \n",
    "    def update_activations(self, cycle):\n",
    "        \"\"\"\n",
    "        Updates unit activations based on current reading cycle.\n",
    "        1. The activation of each text unit in the current cycle is \n",
    "            computed as the sum of the activations spread from each connected \n",
    "            unit based on those units' activation in the previous cycle, \n",
    "            modulated by decay_rate.  \n",
    "        2. Regardless of the outcome, the activations of units in the current \n",
    "            cycle are set to the maximum allowed value.  \n",
    "        3. Finally activations are reduced proportionately based on \n",
    "            memory_capacity.  \n",
    "        Argument:\n",
    "        - cycle: vector of unit indices processed in this cycle\n",
    "        \"\"\"\n",
    "\n",
    "        # spread of activations from previous cycle based on connection weights\n",
    "        # with positive logarithmic change in connection strengths enforced\n",
    "        sigma = torch.tanh(3 * (self.connections-1)) + 1\n",
    "        sigma = sigma # TODO: normalize\n",
    "        self.activations = self.decay_rate * ( sigma @ self.activations.t() ).t().squeeze()\n",
    "\n",
    "        # activations of current cycle units get set to maximum allowed value\n",
    "        self.activations.index_put_([torch.cat(cycle)], self.max_activity)\n",
    "\n",
    "        # activations of all units get set to at least minimum activation\n",
    "        self.activations = torch.maximum(\n",
    "            self.activations, torch.ones_like(self.activations, device=device) * self.min_activity\n",
    "        )\n",
    "\n",
    "        # total activations ensured less or equal to memory capacity\n",
    "        total_activation = torch.sum(self.activations)\n",
    "        if total_activation > self.memory_capacity:\n",
    "            self.activations *= self.memory_capacity / total_activation\n",
    "\n",
    "\n",
    "    def update_connections(self, activations):\n",
    "        \"\"\"\n",
    "        Updates model connection weights based on current unit activations.\n",
    "        Connection strength is accumulated from one cycle to the next as a \n",
    "        function of the activation levels of the connected units. The \n",
    "        learning_rate parameter controls the rate of change, with a high value \n",
    "        representing a higher rate of learning from previous textual \n",
    "        information. Because learning_rate or activation values cannot be \n",
    "        smaller than 0, the connection strength necessarily is above 0, and \n",
    "        changes are incremental.\n",
    "        \"\"\"\n",
    "\n",
    "        self.connections += self.learning_rate * torch.outer(\n",
    "            activations, activations)\n",
    "\n",
    "    def outcome_probabilities(self):\n",
    "        \"\"\"\n",
    "        Current unit recall probabilities given model state.\n",
    "        \"\"\"\n",
    "\n",
    "        activations = torch.pow(self.activations, self.choice_sensitivity)\n",
    "        probabilities = torch.zeros((self.unit_count + 1))\n",
    "        probabilities[0] = min(self.stop_probability_scale * np.exp(\n",
    "            self.recall_total * self.stop_probability_growth), 1.0)\n",
    "\n",
    "        if probabilities[0] < 1:\n",
    "            for unit in range(self.encoding_index):\n",
    "                if unit in self.recall[:self.recall_total]:\n",
    "                    continue\n",
    "                probabilities[unit + 1] = self.activations[unit]\n",
    "            probabilities[1:] *= (\n",
    "                1 - probabilities[0]) / torch.sum(probabilities[1:])\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def draft_probability(self):\n",
    "        \"\"\"\n",
    "        Possible probability based on activations and connections\n",
    "        \"\"\"\n",
    "        return torch.softmax(\n",
    "            self.activations.unsqueeze(0).t() @ (self.connections @ self.activations.t()).unsqueeze(0),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "    def force_recall(self, choice):\n",
    "        \"\"\"\n",
    "        Forces model to recall chosen unit and updates model state.\n",
    "        Here, recall items are 1-indexed, with a choice of 0 indicating a \n",
    "        choice to end retrieval and return to preretrieval model state.\n",
    "        \"\"\"\n",
    "        if not self.retrieving:\n",
    "            self.recall = torch.zeros(self.unit_count)\n",
    "            self.recall_total = 0\n",
    "            self.preretrieval_activations = self.activations\n",
    "            self.retrieving = True\n",
    "\n",
    "        if choice is None:\n",
    "            pass\n",
    "        elif choice == 0:\n",
    "            self.retrieving = False\n",
    "            self.activations = self.preretrieval_activations\n",
    "        else:\n",
    "            self.recall[self.recall_total] = choice - 1\n",
    "            self.recall_total += 1\n",
    "            self.update_activations(np.array([choice]))\n",
    "        return self.recall[:self.recall_total]\n",
    "\n",
    "\n",
    "    def free_recall(self, steps=None):\n",
    "        \"\"\"\n",
    "        Simulates free recall for specified steps based on model state.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.retrieving:\n",
    "            self.recall = torch.zeros(self.unit_count)\n",
    "            self.recall_total = 0\n",
    "            self.preretrieval_activations = self.activations\n",
    "            self.retrieving = True\n",
    "            \n",
    "        # number of items to retrieve is infinite if steps is unspecified\n",
    "        if steps is None:\n",
    "            steps = math.inf\n",
    "        steps = self.recall_total + steps\n",
    "\n",
    "        # at each recall attempt\n",
    "        while self.recall_total < steps:\n",
    "\n",
    "            # compute outcome probabilities, choose based on distribution\n",
    "            outcome_probabilities = self.outcome_probabilities()\n",
    "            if torch.any(outcome_probabilities[1:]):\n",
    "                choice = torch.sum(\n",
    "                    torch.cumsum(outcome_probabilities) < np.random.rand())\n",
    "            else:\n",
    "                choice = 0\n",
    "\n",
    "            # resolve and maybe store outcome\n",
    "            # we stop recall if no choice is made (0)\n",
    "            if choice is None:\n",
    "                pass\n",
    "            elif choice == 0:\n",
    "                self.retrieving = False\n",
    "                self.activations = self.preretrieval_activations\n",
    "            else:\n",
    "                self.recall[self.recall_total] = choice - 1\n",
    "                self.recall_total += 1\n",
    "                self.update_activations(np.array([choice]))\n",
    "        return self.recall[:self.recall_total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_connections = ts(reading_cycles)\n",
    "lsr = LandscapeRevised(init_connections, 0.1, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr.experience([\n",
    "                [ torch.tensor([5])],\n",
    "                [ torch.tensor([6]),torch.tensor([7]) ]\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4668, 0.2730, 0.5279, 0.5367, 0.5704, 0.3599, 1.0000, 1.0000],\n",
       "       device='cuda:0', grad_fn=<MaximumBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsr.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1185, 0.0579, 0.1805, 0.1699, 0.2112, 0.0649, 0.1146, 0.0824],\n",
       "        [0.1240, 0.0816, 0.1586, 0.1530, 0.1738, 0.0872, 0.1216, 0.1002],\n",
       "        [0.1162, 0.0517, 0.1869, 0.1744, 0.2232, 0.0588, 0.1118, 0.0769],\n",
       "        [0.1158, 0.0508, 0.1878, 0.1751, 0.2250, 0.0579, 0.1114, 0.0762],\n",
       "        [0.1143, 0.0477, 0.1912, 0.1774, 0.2316, 0.0548, 0.1097, 0.0733],\n",
       "        [0.1220, 0.0703, 0.1687, 0.1610, 0.1904, 0.0767, 0.1189, 0.0921],\n",
       "        [0.0913, 0.0197, 0.2249, 0.1973, 0.3148, 0.0251, 0.0850, 0.0419],\n",
       "        [0.0913, 0.0197, 0.2249, 0.1973, 0.3148, 0.0251, 0.0850, 0.0419]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsr.draft_probability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0842, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(lsr.draft_probability(), torch.randperm(8, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3], [4], [5, 6, 7]]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reading_cycles1 = [\n",
    "  [\"this is one\", \"of the many\", \"we have\", \"to use\"],\n",
    "  [\"this is another\"],\n",
    "  [\"do you think\", \"this could be\", \"a final?\"]\n",
    "]\n",
    "ic1 = ts(reading_cycles1)\n",
    "cycle_idxs1 = []\n",
    "count = 0\n",
    "for i in range(len(reading_cycles1)):\n",
    "  cycle_idxs1.append([])\n",
    "  cycle_idxs1[i] = []\n",
    "  for j in reading_cycles1[i]:\n",
    "    cycle_idxs1[i].append(count)\n",
    "    count += 1\n",
    "cycle_idxs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4], [5], [6, 7, 8, 9]]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reading_cycles2 = [\n",
    "  [\"look at this graph\", \"said Nickel Back\", \"a long time ago\"],\n",
    "  [\"or is it Nickel back\", \"or Nickle Back\"],\n",
    "  [\"Perhaps\"],\n",
    "  [\"we may never know\", \"said someone\", \"in an anonymous manner\", \"anonymously\"]\n",
    "]\n",
    "ic2 = ts(reading_cycles2)\n",
    "cycle_idxs2 = []\n",
    "count = 0\n",
    "for i in range(len(reading_cycles2)):\n",
    "  cycle_idxs2.append([])\n",
    "  cycle_idxs2[i] = []\n",
    "  for j in reading_cycles2[i]:\n",
    "    cycle_idxs2[i].append(count)\n",
    "    count += 1\n",
    "cycle_idxs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "  [torch.randperm(len(ic1), device=device), ic1, cycle_idxs1],\n",
    "  [torch.randperm(len(ic2), device=device), ic2, cycle_idxs2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandscapeDS(Dataset):\n",
    "  def __init__(self, data):\n",
    "    \"\"\"\n",
    "    data: a Python list of datapoints each containing\n",
    "      - a np array of text unit indices in recall order (true value)\n",
    "      - a matrix of initial connection weights\n",
    "      - a list of lists denoting the reading cycles (convert to list of torch tensors?)\n",
    "    \"\"\"\n",
    "    super(LandscapeDS, self).__init__()\n",
    "    # TODO: some preprocessing of the input data to get only the stuff I need\n",
    "    self.data = data\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    return self.data[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LandscapeDS(data)\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=True) # can't have >1 batch_size as size differs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put this in a Trainer class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss calculation\n",
    "def loss_fn(model, datapoint, criterion=F.cross_entropy):\n",
    "    model.reset(datapoint[1])\n",
    "    model.experience(datapoint[2])\n",
    "\n",
    "    return criterion(model.draft_probability(), datapoint[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, opt, dl):\n",
    "    \"\"\"fit the model to the dl for the specified number of epochs\"\"\"\n",
    "    losses = []\n",
    "    print_interval = epochs // 10 + (epochs < 10)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # setup backprop\n",
    "        model.train()\n",
    "        loss = 0\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # pass through the entire dataset\n",
    "        for datapoint in dl:\n",
    "            loss += loss_fn(model, datapoint)\n",
    "        loss = loss / len(dl)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        model.eval()\n",
    "        # validation stuff\n",
    "\n",
    "        if epoch % print_interval == 0:\n",
    "          print(f\"Epoch {epoch} Loss {loss}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "item_zero = next(iter(dl))\n",
    "lsr = LandscapeRevised(item_zero[1], 0.1, 0.1, 1).to(device)\n",
    "\n",
    "# create optimizer\n",
    "LR = 0.1 # needs to be tuned\n",
    "optimizer = optim.SGD(lsr.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.1905179023742676\n",
      "Epoch 1 Loss 2.190880298614502\n",
      "Epoch 2 Loss 2.1939432621002197\n",
      "Epoch 3 Loss 2.1932473182678223\n",
      "Epoch 4 Loss 2.1896443367004395\n",
      "Epoch 5 Loss 2.1898627281188965\n",
      "Epoch 6 Loss 2.1912450790405273\n",
      "Epoch 7 Loss 2.1911725997924805\n",
      "Epoch 8 Loss 2.193817615509033\n",
      "Epoch 9 Loss 2.192399501800537\n"
     ]
    }
   ],
   "source": [
    "lossses = fit(10, lsr, optimizer, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('max_activity', tensor([1.0104], device='cuda:0')),\n",
       "             ('min_activity', tensor([-0.0007], device='cuda:0')),\n",
       "             ('decay_rate', tensor([0.1524], device='cuda:0')),\n",
       "             ('memory_capacity', tensor([4.9986], device='cuda:0')),\n",
       "             ('learning_rate', tensor([0.9054], device='cuda:0')),\n",
       "             ('semantic_strength', tensor([0.9978], device='cuda:0'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsr.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial default params:\n",
    "max_activity=1.0, \n",
    "min_activity=0.0, \n",
    "decay_rate=0.1, \n",
    "memory_capacity=5.0, \n",
    "learning_rate=0.9, \n",
    "semantic_strength=1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
