{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data obtained from the Brown-Schmidt lab for this research requires extensive extra preprocessing to be suitable for downstream analyses. Here we do that by:\n",
    "\n",
    "- Removing irrelevant boilerplate\n",
    "- Putting text into a maintainable file format\n",
    "- Extracting serial order of study idea units from texts and researcher segmentations/correspondences\n",
    "- Pairing extracted trial data with semantic similarity data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dataset Overview\n",
    "We render an overview of the dataset prepared for our publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Recall for narratives, if split into idea units &ndash; \"meaningful\n",
       "chunks of information that convey a piece of the narrative\" &ndash; that\n",
       "are numbered according to chronological order, can be examined using\n",
       "analytic techniques developed for free and serial list recall tasks.\n",
       "This framework enables direct comparison between ideas, assumptions, and\n",
       "models applied to understand how people remember sequences such as word\n",
       "lists and those used to understand memory for narrative texts. To\n",
       "support analysis of narrative recall this way, we considered a dataset\n",
       "collected, preprocessed, and presented by Cutler et al. (2019). In\n",
       "corresponding experiments, research participants read 6 distinct short\n",
       "stories. Upon reading a story, participants performed immediate free\n",
       "recall of the narrative twice. Three weeks later, participants performed\n",
       "free recall of each narrative again. Each recall period was limited to\n",
       "five minutes. Following data collection, a pair of research assistants\n",
       "in the Brown-Schmidt laboratory were each instructed to independently\n",
       "split stories and participant responses into idea units as defined\n",
       "above, and to identify correspondences between idea units in participant\n",
       "responses and corresponding studied stories reflecting recall. Following\n",
       "this initial preprocessing, research assistants then compared and\n",
       "discussed their results and recorded consensus decisions regarding the\n",
       "segmentation and correspondence of idea units across the dataset.\n",
       "Further analysis focused on the sequences of story idea units recalled\n",
       "by participants on each trial as tracked by these researchers.\n",
       "\n",
       "<div id=\"refs\" class=\"references csl-bib-body hanging-indent\"\n",
       "markdown=\"1\" line-spacing=\"2\">\n",
       "\n",
       "<div id=\"ref-cutler2019narrative\" class=\"csl-entry\" markdown=\"1\">\n",
       "\n",
       "Cutler, R., Palan, J., Polyn, S., & Brown-Schmidt, S. (2019). Semantic\n",
       "and temporal structure in memory for narratives: A benefit for\n",
       "semantically congruent ideas. *Context and Episodic Memory Symposium*.\n",
       "\n",
       "</div>\n",
       "\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def render_tex(tex_path, bib_path, csl_path):\n",
    "    result = !pandoc -C --ascii {tex_path} -f latex -t markdown_mmd --bibliography {bib_path} --csl {csl_path}\n",
    "    return Markdown('\\n'.join(result))\n",
    "\n",
    "render_tex('writing/BrownSchmidt_Dataset.tex', 'writing/references.bib', 'writing/main/apa.csl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardizing Text Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data in `raw`, we produce in `texts` one subdirectory for each passage (with passage contents at base) and in each subdirectory, one file for each recall period. Each file will contain only the recalled text associated with a particular passage, subject, and recall period and be labeled accordingly (e.g. as `Supermarket_1_1.txt`). At the base of `texts`, the text of the source passages will each be included as separate files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start with some initial dependencies and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import pathlib\n",
    "import docxpy\n",
    "import ftfy\n",
    "\n",
    "# key paths\n",
    "source_directory = os.path.join('data', 'raw')\n",
    "target_directory = os.path.join('data', 'texts')\n",
    "\n",
    "source_names = ['Fisherman', 'Supermarket', 'Flight', 'Cat', 'Fog', 'Beach']\n",
    "source_titles = ['where does susie go at noon?']\n",
    "title_tags = [['''man and the bear'''], ['''act of kindness'''], \n",
    "              [\"\"\"a man can’t just sit\"\"\", \"a man just can’t sit\"], \n",
    "              [\"where does susie go at noon?\"], [\"fog: a maine t\"], \n",
    "              [\"day at the beach\"]]\n",
    "author_tags = ['author unknown', 'anonymous', 'chris holm', 'adapted from',\n",
    "               'unknown', 'anonymous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we create directories in our file system to organize preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pooled subdirectory if one doesn't already exist\n",
    "if not os.path.isdir(target_directory):\n",
    "    os.mkdir(target_directory)\n",
    "\n",
    "# generate subdirectory for each passage\n",
    "for source_name in source_names:\n",
    "    passage_path = os.path.join(target_directory, source_name)\n",
    "    if not os.path.isdir(passage_path):\n",
    "        os.mkdir(passage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess raw `docx` files and store as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pt1 written recall file, extract text and remove boilerplate, \n",
    "# and save to correct location in `pooled`\n",
    "for path, subdirs, files in os.walk(os.path.join(\n",
    "    source_directory, 'recall', 'Written Recall Part 1')):\n",
    "    for name in files:\n",
    "        recall_path = str(pathlib.PurePath(path, name))\n",
    "        \n",
    "        # extract text and remove boilerplate\n",
    "        recall_text = '\\n'.join(\n",
    "            docxpy.process(recall_path).split('\\n')[1:]).strip()\n",
    "        passage_index = recall_path[-9:-8]\n",
    "        subject_index = recall_path.split(name)[0][-3:-1]\n",
    "        phase_index = recall_path[-7:-6]\n",
    "        targetname = '{}_{}_{}.txt'.format(\n",
    "            source_names[int(passage_index)-1], int(subject_index), phase_index)\n",
    "        \n",
    "        # handle special cases??\n",
    "        recall_text = recall_text.replace(\n",
    "            'vbeach', 'beach').replace('Susie gp at noon', 'Susie go at noon')\n",
    "        \n",
    "        # filter out source titles from recall data\n",
    "        if any([each in recall_text[:recall_text.find(\n",
    "            '.')].lower() for each in title_tags[int(passage_index)-1]]):\n",
    "            if len(recall_text[:recall_text.find('\\n')]) < 100:\n",
    "                recall_text = recall_text[recall_text.find('\\n'):].strip()\n",
    "                \n",
    "        # filter out source authors from recall data\n",
    "        if (recall_text[:len(author_tags[int(\n",
    "            passage_index)-1])].lower() == author_tags[int(passage_index)-1]):\n",
    "            recall_text = recall_text[recall_text.find('\\n'):].strip()\n",
    "            \n",
    "        # clean the data\n",
    "        recall_text = ftfy.fix_text(recall_text)\n",
    "            \n",
    "        # save to correct location in pooled\n",
    "        with open(\n",
    "            os.path.join(target_directory, source_names[int(passage_index)-1], \n",
    "                         targetname), 'w', encoding='utf-8') as f:\n",
    "            f.write(recall_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 and Part 2 data were collected in slightly different contexts, so they are preprocessed a little differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pt2 written recall file, extract text and remove boilerplate, \n",
    "# and save to correct location in `pooled`\n",
    "for path, subdirs, files in os.walk(\n",
    "    os.path.join(source_directory, 'recall', 'Written Recall Part 2')):\n",
    "    for name in files:\n",
    "        recall_path = str(pathlib.PurePath(path, name))\n",
    "        \n",
    "        # identify correct location in pooled\n",
    "        passage_index = recall_path[-7:-6]\n",
    "        subject_index, phase_index =  recall_path.split(name)[0][-3:-1], 3\n",
    "        if len(passage_index.strip()) == 0:\n",
    "            continue\n",
    "        targetname = '{}_{}_{}.txt'.format(\n",
    "            source_names[int(passage_index)-1], int(subject_index), phase_index)\n",
    "\n",
    "        # extract text and remove boilerplate\n",
    "        boilerplate = 'You have 5 minutes to type the story you just read for memory. There is no word limit. Please write as much as you can remember.'\n",
    "        recall_text = docxpy.process(\n",
    "            recall_path).replace(boilerplate, '').strip()\n",
    "        recall_text = '\\n'.join(recall_text.split('\\n')[1:]).strip()\n",
    "        \n",
    "        # clean text\n",
    "        recall_text = ftfy.fix_text(recall_text)\n",
    "        \n",
    "        # save to correct location\n",
    "        with open(os.path.join(\n",
    "            target_directory, source_names[int(passage_index)-1], \n",
    "            targetname), 'w', encoding='utf-8') as f:\n",
    "            f.write(recall_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result is an organized directory of text representations of participant responses absent methodology-specific details such as the content of the recall prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
