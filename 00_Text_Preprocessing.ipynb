{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data obtained from the Brown-Schmidt lab for this research requires extensive extra preprocessing to be suitable for downstream analyses. Here we do that by:\n",
    "\n",
    "- Removing irrelevant boilerplate\n",
    "- Putting text into a maintainable file format\n",
    "- Extracting serial order of study idea units from texts and researcher segmentations/correspondences\n",
    "- Pairing extracted trial data with semantic similarity data\n",
    "\n",
    "Tabnine::config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "We render an overview of the dataset prepared for our publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Recall for narratives, if split into idea units &ndash; \"meaningful\n",
       "chunks of information that convey a piece of the narrative\" &ndash; that\n",
       "are numbered according to chronological order, can be examined using\n",
       "analytic techniques developed for free and serial list recall tasks.\n",
       "This framework enables direct comparison between ideas, assumptions, and\n",
       "models applied to understand how people remember sequences such as word\n",
       "lists and those used to understand memory for narrative texts. To\n",
       "support analysis of narrative recall this way, we considered a dataset\n",
       "collected, preprocessed, and presented by Cutler et al. (2019). In\n",
       "corresponding experiments, research participants read 6 distinct short\n",
       "stories. Upon reading a story, participants performed immediate free\n",
       "recall of the narrative twice. Three weeks later, participants performed\n",
       "free recall of each narrative again. Each recall period was limited to\n",
       "five minutes. Following data collection, a pair of research assistants\n",
       "in the Brown-Schmidt laboratory were each instructed to independently\n",
       "split stories and participant responses into idea units as defined\n",
       "above, and to identify correspondences between idea units in participant\n",
       "responses and corresponding studied stories reflecting recall. Following\n",
       "this initial preprocessing, research assistants then compared and\n",
       "discussed their results and recorded consensus decisions regarding the\n",
       "segmentation and correspondence of idea units across the dataset.\n",
       "Further analysis focused on the sequences of story idea units recalled\n",
       "by participants on each trial as tracked by these researchers.\n",
       "\n",
       "<div id=\"refs\" class=\"references csl-bib-body hanging-indent\"\n",
       "markdown=\"1\" line-spacing=\"2\">\n",
       "\n",
       "<div id=\"ref-cutler2019narrative\" class=\"csl-entry\" markdown=\"1\">\n",
       "\n",
       "Cutler, R., Palan, J., Polyn, S., & Brown-Schmidt, S. (2019). Semantic\n",
       "and temporal structure in memory for narratives: A benefit for\n",
       "semantically congruent ideas. *Context and Episodic Memory Symposium*.\n",
       "\n",
       "</div>\n",
       "\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def render_tex(tex_path, bib_path, csl_path):\n",
    "    result = !pandoc -C --ascii {tex_path} -f latex -t markdown_mmd --bibliography {bib_path} --csl {csl_path}\n",
    "    return Markdown('\\n'.join(result))\n",
    "\n",
    "render_tex('writing/BrownSchmidt_Dataset.tex', 'writing/references.bib', 'writing/main/apa.csl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Text Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data in `raw`, we produce in `texts` one subdirectory for each passage (with passage contents at base) and in each subdirectory, one file for each recall period. Each file will contain only the recalled text associated with a particular passage, subject, and recall period and be labeled accordingly (e.g. as `Supermarket_1_1.txt`). At the base of `texts`, the text of the source passages will each be included as separate files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start with some initial dependencies and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import pathlib\n",
    "import docxpy\n",
    "import ftfy\n",
    "\n",
    "# key paths\n",
    "source_directory = os.path.join('data', 'raw')\n",
    "target_directory = os.path.join('data', 'texts')\n",
    "\n",
    "source_names = ['Fisherman', 'Supermarket', 'Flight', 'Cat', 'Fog', 'Beach']\n",
    "source_titles = ['where does susie go at noon?']\n",
    "title_tags = [['''man and the bear'''], ['''act of kindness'''], \n",
    "              [\"\"\"a man can’t just sit\"\"\", \"a man just can’t sit\"], \n",
    "              [\"where does susie go at noon?\"], [\"fog: a maine t\"], \n",
    "              [\"day at the beach\"]]\n",
    "author_tags = ['author unknown', 'anonymous', 'chris holm', 'adapted from',\n",
    "               'unknown', 'anonymous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we create directories in our file system to organize preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pooled subdirectory if one doesn't already exist\n",
    "if not os.path.isdir(target_directory):\n",
    "    os.mkdir(target_directory)\n",
    "\n",
    "# generate subdirectory for each passage\n",
    "for source_name in source_names:\n",
    "    passage_path = os.path.join(target_directory, source_name)\n",
    "    if not os.path.isdir(passage_path):\n",
    "        os.mkdir(passage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess raw `docx` files and store as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pt1 written recall file, extract text and remove boilerplate, \n",
    "# and save to correct location in `pooled`\n",
    "for path, subdirs, files in os.walk(os.path.join(\n",
    "    source_directory, 'recall', 'Written Recall Part 1')):\n",
    "    for name in files:\n",
    "        recall_path = str(pathlib.PurePath(path, name))\n",
    "        \n",
    "        # extract text and remove boilerplate\n",
    "        recall_text = '\\n'.join(\n",
    "            docxpy.process(recall_path).split('\\n')[1:]).strip()\n",
    "        passage_index = recall_path[-9:-8]\n",
    "        subject_index = recall_path.split(name)[0][-3:-1]\n",
    "        phase_index = recall_path[-7:-6]\n",
    "        targetname = '{}_{}_{}.txt'.format(\n",
    "            source_names[int(passage_index)-1], int(subject_index), phase_index)\n",
    "        \n",
    "        # handle special cases??\n",
    "        recall_text = recall_text.replace(\n",
    "            'vbeach', 'beach').replace('Susie gp at noon', 'Susie go at noon')\n",
    "        \n",
    "        # filter out source titles from recall data\n",
    "        if any([each in recall_text[:recall_text.find(\n",
    "            '.')].lower() for each in title_tags[int(passage_index)-1]]):\n",
    "            if len(recall_text[:recall_text.find('\\n')]) < 100:\n",
    "                recall_text = recall_text[recall_text.find('\\n'):].strip()\n",
    "                \n",
    "        # filter out source authors from recall data\n",
    "        if (recall_text[:len(author_tags[int(\n",
    "            passage_index)-1])].lower() == author_tags[int(passage_index)-1]):\n",
    "            recall_text = recall_text[recall_text.find('\\n'):].strip()\n",
    "            \n",
    "        # clean the data\n",
    "        recall_text = ftfy.fix_text(recall_text)\n",
    "            \n",
    "        # save to correct location in pooled\n",
    "        with open(\n",
    "            os.path.join(target_directory, source_names[int(passage_index)-1], \n",
    "                         targetname), 'w', encoding='utf-8') as f:\n",
    "            f.write(recall_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 and Part 2 data were collected in slightly different contexts, so they are preprocessed a little differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pt2 written recall file, extract text and remove boilerplate, \n",
    "# and save to correct location in `pooled`\n",
    "for path, subdirs, files in os.walk(\n",
    "    os.path.join(source_directory, 'recall', 'Written Recall Part 2')):\n",
    "    for name in files:\n",
    "        recall_path = str(pathlib.PurePath(path, name))\n",
    "        \n",
    "        # identify correct location in pooled\n",
    "        passage_index = recall_path[-7:-6]\n",
    "        subject_index, phase_index =  recall_path.split(name)[0][-3:-1], 3\n",
    "        if len(passage_index.strip()) == 0:\n",
    "            continue\n",
    "        targetname = '{}_{}_{}.txt'.format(\n",
    "            source_names[int(passage_index)-1], int(subject_index), phase_index)\n",
    "\n",
    "        # extract text and remove boilerplate\n",
    "        boilerplate = 'You have 5 minutes to type the story you just read for memory. There is no word limit. Please write as much as you can remember.'\n",
    "        recall_text = docxpy.process(\n",
    "            recall_path).replace(boilerplate, '').strip()\n",
    "        recall_text = '\\n'.join(recall_text.split('\\n')[1:]).strip()\n",
    "        \n",
    "        # clean text\n",
    "        recall_text = ftfy.fix_text(recall_text)\n",
    "        \n",
    "        # save to correct location\n",
    "        with open(os.path.join(\n",
    "            target_directory, source_names[int(passage_index)-1], \n",
    "            targetname), 'w', encoding='utf-8') as f:\n",
    "            f.write(recall_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result is an organized directory of text representations of participant responses absent methodology-specific details such as the content of the recall prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Sequence Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our analyses don't work directly on the text data we preprocess above. Instead, we want something formatted more like the traditional object of free recall modeling: vectors tracking the order in which items were recalled in each trial.\n",
    "\n",
    "What kind of information do we want stored about each trial for subsequent analyses?\n",
    "\n",
    "For downstream interpretability:\n",
    "- `source_units`, text representation of each idea unit in the original story\n",
    "- `response_units`, text representation of each idea unit in participant's recall response\n",
    "\n",
    "To support simulation/prediction:\n",
    "- `trials`, the serial index of the source unit matched to the serial index of each corresponding response unit represented as a recall sequence; 0 for termination\n",
    "- `cycles`, vector of integers grouping successive items into a cycle based on sentence-structure of studied narratives; 0 for termination.\n",
    "- `similarities`, similarity matrix between each source unit to one another, a common parameter for models of narrative comprehension and memory. 0 diagonal.\n",
    "\n",
    "`cycles` and `source_units` are story-specific. We'll want a vector that codes for each trial the relevant story to simulate along with these structures. We'll similarly want vector that codes the subject and session index for each trial.\n",
    "\n",
    "Stick all of these inside a single MAT file or whatever. Consider pregenerating the psifr dataframe as well.\n",
    "\n",
    "Human raters have gotten us most of what we want in the spreadsheet at `data/raw/Narrative Recall Data.xlsx`. Most extra preprocessing is devoted to sorting source units into specific cycles and tracking response units that raters did not match to a specific source unit - something unlikely to matter for most downstream analyses but still worth keeping up with (and we already have most of the code for it anyway from previous projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# key paths\n",
    "source_directory = os.path.join('data', 'raw')\n",
    "text_directory = os.path.join('data', 'texts')\n",
    "target_directory = os.path.join('data', 'sequences', 'human')\n",
    "\n",
    "# names for relevant passages\n",
    "passage_names = ['Fisherman', 'Supermarket', 'Flight', 'Cat', 'Fog', 'Beach']\n",
    "\n",
    "# we use the original xlsx\n",
    "data = pd.read_excel(os.path.join(\n",
    "    source_directory, 'Narrative Recall Data.xlsx'), \n",
    "                     list(range(22)), engine='openpyxl')\n",
    "\n",
    "data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "class TextSimilarity(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"stsb-distilbert-base\"):\n",
    "        super(TextSimilarity, self).__init__()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # get the model and tokenizer, assuming we are using DistilBert\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.to(device)\n",
    "  \n",
    "    @staticmethod\n",
    "    def cosine_similarity(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Custom implementation of cosine similarities\n",
    "        \"\"\"\n",
    "        norm = x.norm(dim=-1).unsqueeze(0)\n",
    "\n",
    "        # this is the formula for cosine similarities in a symmetric matrix\n",
    "        return x @ x.t() / (norm.t() @ norm)\n",
    "\n",
    "    def forward(self, cycles: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assumes input is a list of lists of sentences/text units\n",
    "        i.e. a List of Lists of strings\n",
    "        \"\"\"\n",
    "        embeddings = torch.cat([self.model.encode(i, convert_to_tensor=True) for i in cycles])\n",
    "        init_connections = self.cosine_similarity(embeddings)\n",
    "        return init_connections\n",
    "    \n",
    "ts = TextSimilarity(\"stsb-distilbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story Information\n",
    "- Strings identifying idea units within each story\n",
    "- Semantic similarity matrix between source idea units\n",
    "- Cycles grouping source idea units based on co-occurence in the same sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cycles = []\n",
    "all_source_units = []\n",
    "all_similarities = []\n",
    "story_sequence = []\n",
    "\n",
    "for trial_index, trial in data[0].groupby(['story', 'timeTest']):\n",
    "    \n",
    "    # we only consider each story once\n",
    "    if trial['timeTest'].values[0] > 1:\n",
    "        continue\n",
    "    \n",
    "    # identify story\n",
    "    story_index = trial['story'].values[0]\n",
    "    story_sequence.append(story_index)\n",
    "    print(story_index)\n",
    "    \n",
    "    # source units are reproduced perfectly in xlsx file\n",
    "    source_units = [each for each in list(trial['origText']) if type(each) == str]\n",
    "    \n",
    "    # collect relevant text\n",
    "    with open(os.path.join(\n",
    "        text_directory, passage_names[story_index-1] + '.txt'), encoding='utf8') as f:\n",
    "        story_text = f.read()\n",
    "        \n",
    "    # identify discrete sentences in the text so we can sort units into cycles\n",
    "    sentences = [each for each in nlp(story_text).sents if len(each) > 1]\n",
    "    \n",
    "    # build cycle vector identifying the number of idea units per sentence\n",
    "    cycles = []\n",
    "    last = 0\n",
    "    counter = 0\n",
    "    for unit in source_units:\n",
    "\n",
    "        for sentence_index, sentence in enumerate(sentences):\n",
    "            if unit in sentence.text:\n",
    "\n",
    "                if sentence_index == last:\n",
    "                    counter += 1\n",
    "                else: \n",
    "                    cycles.append(counter)\n",
    "                    last = sentence_index\n",
    "                    counter = 1\n",
    "                break\n",
    "                \n",
    "    # track semantic similarity between each source unit\n",
    "    similarities = ts([source_units]).detach().tolist()\n",
    "    \n",
    "    all_cycles.append(cycles)\n",
    "    all_similarities.append(similarities)\n",
    "    all_source_units.append(source_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we want a `trials` array with each row indicating the order of recalled source units for each trial. We also want for each trial the strings identifying the idea units in participants' responses. \n",
    "\n",
    "We also want to build vectors coding for each trial the story, timeTest, and subject corresponding to each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_correspondences = []\n",
    "all_response_units = []\n",
    "all_subject = []\n",
    "all_story = []\n",
    "all_timeTest = []\n",
    "\n",
    "# path format for subject text data,\n",
    "text_path = os.path.join(text_directory, '{}', '{}_{}_{}.txt')\n",
    "\n",
    "# consider each unique trial\n",
    "for subject_index, subject in enumerate(data):\n",
    "    print(subject)\n",
    "    for trial_index, trial in data[subject].groupby(['story', 'timeTest']):\n",
    "        \n",
    "        # identify story, timeTest (we already have subject_index)\n",
    "        story_index = trial['story'].values[0]\n",
    "        timeTest = trial['timeTest'].values[0]\n",
    "        passage_name = passage_names[story_index-1]\n",
    "        \n",
    "        # grab preprocessed textual representation of participant response\n",
    "        # so we can idea units for unrecalled response units\n",
    "        response_text_path = text_path.format(\n",
    "            passage_name, passage_name, subject_index+1, trial_index[1])\n",
    "        \n",
    "        try:\n",
    "            with open(response_text_path, encoding='utf8') as f:\n",
    "                raw_response = f.read()\n",
    "        except FileNotFoundError:\n",
    "            print('Could not find:', response_text_path)\n",
    "            continue\n",
    "        \n",
    "        # for idea unit identification, start with initial units coded in xlsv\n",
    "        # as well as the sentences in the raw text\n",
    "        initial_units = [each for each in list(\n",
    "            trial['recText']) if each and type(each) == str]\n",
    "        response_sentences = [each for each in nlp(raw_response).sents if any(char.isalnum() for char in each.text)]\n",
    "        \n",
    "        # use line breaks to perform segmentation, discarding pre-existing ones\n",
    "        response_text = raw_response.replace('\\n', '\\t') \n",
    "        \n",
    "        # segment by human-coded units then any non-overlapping sentence units\n",
    "        for unit in initial_units:\n",
    "            response_text = response_text.replace(unit, f'\\n{unit}\\n')\n",
    "        for sentence in response_sentences:\n",
    "            response_text = response_text.replace(\n",
    "                sentence.text.replace('\\n', ''), f'\\n{sentence.text}\\n')\n",
    "        \n",
    "        # using this initial split, build list of response idea units\n",
    "        # we'll ignore units light in content (no alphanumeric, etc)\n",
    "        response_units = []\n",
    "        correspondences = []\n",
    "        \n",
    "        for unit in response_text.split('\\n'):\n",
    "\n",
    "            # reject units that have no alphanumeric characters\n",
    "            if not any(char.isalnum() for char in unit):\n",
    "                continue\n",
    "                \n",
    "            # identify index of source unit associated w/ this proposed recall unit\n",
    "            # if unit is unmatched to a source unit, code as -1\n",
    "            # reserve 0 for termination\n",
    "            if unit in initial_units:\n",
    "                correspondence = int(trial.loc[trial['recText'] == unit]['serialPos'])\n",
    "            else:\n",
    "                correspondence = -1\n",
    "            response_units.append(' '.join([word.text for word in nlp(unit)]).lower())\n",
    "            correspondences.append(correspondence)\n",
    "            \n",
    "        all_correspondences.append(correspondences)\n",
    "        all_response_units.append(response_units)\n",
    "        all_subject.append(subject_index)\n",
    "        all_timeTest.append(timeTest)\n",
    "        all_story.append(story_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage\n",
    "For flexibility, we'll go for a simple JSON representation for now and leave further processing for the Data Preparation notebook or other downstream pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = {}\n",
    "\n",
    "# story information\n",
    "result['cycles'] = all_cycles\n",
    "result['source_units'] = all_source_units\n",
    "result['similarities'] = all_similarities\n",
    "result['story_names'] = passage_names\n",
    "\n",
    "# response information\n",
    "result['subject'] = all_subject\n",
    "result['response_units'] = all_response_units\n",
    "result['story'] = [int(each) for each in all_story]\n",
    "result['timeTest'] = [int(each) for each in all_timeTest]\n",
    "result['trials'] = [[int(each) for each in row] for row in all_correspondences]\n",
    "#print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, each in enumerate(result['response_units']):\n",
    "    print(result['subject'][index], result['story'][index], result['timeTest'][index])\n",
    "    print(each)\n",
    "    print(result['trials'][index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
