# quotations for further investigation

## Literature review

For a long period of time, the works of Ekman and Friesen on facial expressions and emotions signaling deception, along with their universality across cultures informed deception research [15]. 

Individual differences in the ability to detect deception as reported in Aamodt et al.’s 2006 meta-analysis

Zuckerman et al. proposed the four-factor theory, describing four psychological factors that contribute to deception clues: generalized arousal, felt emotion, cogni- tive load and strategy to appear truthful [62]. These are observed through the use of more pauses, higher vocal pitch, gestures due to guilt or fear, an increase in cognitive load causing them to become more inconsistent in their statements and the use of more formal speech to appear truthful.

Another interesting theory related to our work is the Interpersonal Deception Theory (IDT), which was developed by Buller and Burgoon [7]. They focused on the dynamic, interactive and dyadic nature of deceptive communication between the sender and receiver which stands in contrast to non-interactive speech. They posit that both parties are actively participating in the exchange with their own individual goals and thus, are constantly adapting based on the other person’s response. 

For example, in a meta-analysis of 158 cues to de- ception from 116 research studies, DePaulo et al. found 23 cues with large effect sizes on differentiating between truth and lies – 21 of which were verbal and 2 were non- verbal [14]. They found that nonverbal immediacy cues like eye contact and fidgeting did not differentially predict deception but verbal cues such as the level of detail and emotional content of statements were informative. 

Hauch et al. performed a meta-study of 79 linguistic cues, showing significant effect sizes on certain linguistic categories [25]. Some of their findings include the usage of fewer first person pronouns, the ex- pression of more negative emotions through negations and terms related to anger and the lower use of perceptual and sensory language by liars compared to truth-tellers.

Of published work, Ott et al. explored generalized approaches for identifying online deceptive opinion spam, or fake reviews made to affect consumer purchase decisions by analyzing the general difference in language used in truthful and deceptive TripAdvisor and Yelp reviews [35]. 

Newman et al. created a corpus by
collecting and transcribing interviews in which participants were asked to lie or tell the truth about topics such as abortion, feelings about friends, crime and the death penalty [41].

Some other researchers like Forniciari et al. studied real-life high-stakes situations such as Italian court hearings and found that detection above chance-level is possible in real-life situations [19].

A number of studies have utilized the Columbia- SRI-Colorado (CSC) Corpus of Deceptive Speech, which employs a fake interview paradigm where subjects would attempt to convince the interviewer by mixing true and false information. Hirschberg et al. later extended this to study the variation in deception patterns across gender and native language using the Columbia X-Cultural Deception (CXD) Corpus [33].

Various classification attempts have been done with these corpora using acoustic-prosodic, lexical and subject-dependent cues [32, 38, 17].

One of the few major works that used Mafia games to study deception was conducted by Zhou and Sung, where they collected 1192 Mafia games from a popular Chinese website [60]. Their findings on influential linguistic cues are contradictory to previous research findings from American researchers, which were later said to be caused by the cross-cultural differences between Chinese and American Mafia players in terms of language usage, along with the inconsistencies in game set up.

Among the most useful lexical analysis tools are Linguistic Inquiry and Word Count (LIWC), Dictionary of Affect in Language (DAL), part-of-speech (POS) and n-gram. Linguistic Inquiry and Word Count (LIWC) is psycho-social dictionary that groups words into function word categories, as well as psychologically motivated categories [43]. The Dictionary of Affect in Language includes over 348,000 words quantified for emotional undertones such as Pleasantness, Activation and Imagery [54].

In addition, some studies have used statement analysis, an approach that combines lexical and syntactic features [49, 52]. 

Various other studies combine n-gram, a contiguous sequence of n items from a sample of text, with other features to detect deception [3, 18].

Automated text-based detection has provided a more reliable alternative to es- tablished tools such as the polygraph test, Scientific Content Analysis (SCAN) and Behavioral Analysis Interview (BAI).

Human judgment is laced with a whole range of personal and exte- rior biases, including the tendency of being truth biased, where they are more likely to judge statements as being true than false.

Secondly, several studies have proven that trained professionals do not perform better than untrained individuals, such as students and novices, and in fact, sometimes perform worse [31, 37].

Classification in most studies perform at around 60-70% accuracy, which is slightly higher than human performance. Fuller et al. used five classification models: Mul- tilayer Perceptron, Radial Basis function, Random Forest, Na ̈ıve Bayes and Support Vector Machine with three cue groups and achieved an accuracy level of 76% [21]. 

Newman et al. used LIWC on a corpus of false and truthful opinions on five contro- versial topics and achieved an accuracy level of 61% overall with a logistic regression model [41]

Simiarly, using a TripAdvisor hotel review dataset, Ott et al. achieved a classification accuracy of 89.8% by combining LIWC and bigrams on a SVM model [35].

Feng at al. later improved on this result using deep syntactic features from Prob- abilistic Context Free Grammar (PCFG) parse trees, increasing the accuracy of an SVM classifier to 91.2% [18]. 

Some early results have proven promising: Kr- ishnamurthy et al. combined features from video, audio, text and Micro-Expression features and detected deception in real-life videos with an accuracy of 96.14% [29].

Studies by Burgoon et al. and DePaulo et al. have shown that deceivers use fewer words than truth tellers in spoken communication [9, 14].

However, in contrast, two other studies using a computer-mediated setting in which participants exchanged synchronous messages found that deceivers used more words than non-deceivers [24, 61].

Studies have consistently shown that liars use fewer first-person pronouns and self-references, along with more second-person pronouns than truth-tellers [14, 61].

Findings on the use of third person-pronouns are mixed, with studies showing both higher and lower use by deceivers [24, 41]. 

Next, DePaulo et al. studied small “everyday” lies in diary entries and found that people felt guilty and uncomfortably while lying and immediately afterward [13]. 

Previous work has found reduced content diversity, shorter sentences, higher Flesh-Kincaid readability scores which indicates easier readability and more redundancy such as the use of repeated words by deceptive players [8, 9, 61]. 

In natural language processing, many neural networks based on dense vector representations, known as word embeddings, for text classification have been proposed and shown to outperform traditional methods [30, 56, 58].

## Techniques
Originally developed for computer vision, convolution neural networks (CNN) use layers with convolving filters that are applied to local features.

What does convolving filter mean?

We consider the common architecture where the model has three basic building blocks: convolutional, pooling and fully-connected layers.

## Questions
