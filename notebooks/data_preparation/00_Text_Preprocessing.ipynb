{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0e0f2b",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f0702",
   "metadata": {},
   "source": [
    "The raw data obtained from the Brown-Schmidt lab for this research requires extensive extra preprocessing to be suitable for downstream analyses. Here we do that by:\n",
    "\n",
    "- Removing irrelevant boilerplate\n",
    "- Putting text into a maintainable file format\n",
    "- Extracting serial order of study idea units from texts and researcher segmentations/correspondences\n",
    "- Pairing extracted trial data with semantic similarity data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec194211",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "We render an overview of the dataset prepared for our publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92738841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def render_tex(tex_path, bib_path, csl_path):\n",
    "    result = !pandoc -C --ascii {tex_path} -f latex -t markdown_mmd --bibliography {bib_path} --csl {csl_path}\n",
    "    return Markdown('\\n'.join(result))\n",
    "\n",
    "render_tex('writing/BrownSchmidt_Dataset.tex', 'writing/references.bib', 'writing/main/apa.csl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6ee17",
   "metadata": {},
   "source": [
    "## Standardizing Text Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f29d4",
   "metadata": {},
   "source": [
    "Using the data in `raw`, we produce in `texts` one subdirectory for each passage (with passage contents at base) and in each subdirectory, one file for each recall period. Each file will contain only the recalled text associated with a particular passage, subject, and recall period and be labeled accordingly (e.g. as `Supermarket_1_1.txt`). At the base of `texts`, the text of the source passages will each be included as separate files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2f952",
   "metadata": {},
   "source": [
    "### We start with some initial dependencies and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import pathlib\n",
    "import docxpy\n",
    "import ftfy\n",
    "\n",
    "# key paths\n",
    "source_directory = os.path.join('data', 'raw')\n",
    "target_directory = os.path.join('data', 'texts')\n",
    "\n",
    "source_names = ['Fisherman', 'Supermarket', 'Flight', 'Cat', 'Fog', 'Beach']\n",
    "source_titles = ['where does susie go at noon?']\n",
    "title_tags = [['''man and the bear'''], ['''act of kindness'''], \n",
    "              [\"\"\"a man can’t just sit\"\"\", \"a man just can’t sit\"], \n",
    "              [\"where does susie go at noon?\"], [\"fog: a maine t\"], \n",
    "              [\"day at the beach\"]]\n",
    "author_tags = ['author unknown', 'anonymous', 'chris holm', 'adapted from',\n",
    "               'unknown', 'anonymous']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652f6d8",
   "metadata": {},
   "source": [
    "### Next we create directories in our file system to organize preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pooled subdirectory if one doesn't already exist\n",
    "if not os.path.isdir(target_directory):\n",
    "    os.mkdir(target_directory)\n",
    "\n",
    "# generate subdirectory for each passage\n",
    "for source_name in source_names:\n",
    "    passage_path = os.path.join(target_directory, source_name)\n",
    "    if not os.path.isdir(passage_path):\n",
    "        os.mkdir(passage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f49e80",
   "metadata": {},
   "source": [
    "### Preprocess raw `docx` files and store as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad46f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pt1 written recall file, extract text and remove boilerplate, \n",
    "# and save to correct location in `pooled`\n",
    "for path, subdirs, files in os.walk(os.path.join(\n",
    "    source_directory, 'recall', 'Written Recall Part 1')):\n",
    "    for name in files:\n",
    "        recall_path = str(pathlib.PurePath(path, name))\n",
    "        \n",
    "        # extract text and remove boilerplate\n",
    "        recall_text = '\\n'.join(\n",
    "            docxpy.process(recall_path).split('\\n')[1:]).strip()\n",
    "        passage_index = recall_path[-9:-8]\n",
    "        subject_index = recall_path.split(name)[0][-3:-1]\n",
    "        phase_index = recall_path[-7:-6]\n",
    "        targetname = '{}_{}_{}.txt'.format(\n",
    "            source_names[int(passage_index)-1], int(subject_index), phase_index)\n",
    "        \n",
    "        # handle special cases??\n",
    "        recall_text = recall_text.replace(\n",
    "            'vbeach', 'beach').replace('Susie gp at noon', 'Susie go at noon')\n",
    "        \n",
    "        # filter out source titles from recall data\n",
    "        if any([each in recall_text[:recall_text.find(\n",
    "            '.')].lower() for each in title_tags[int(passage_index)-1]]):\n",
    "            if len(recall_text[:recall_text.find('\\n')]) < 100:\n",
    "                recall_text = recall_text[recall_text.find('\\n'):].strip()\n",
    "                \n",
    "        # filter out source authors from recall data\n",
    "        if (recall_text[:len(author_tags[int(\n",
    "            passage_index)-1])].lower() == author_tags[int(passage_index)-1]):\n",
    "            recall_text = recall_text[recall_text.find('\\n'):].strip()\n",
    "            \n",
    "        # clean the data\n",
    "        recall_text = ftfy.fix_text(recall_text)\n",
    "            \n",
    "        # save to correct location in pooled\n",
    "        with open(\n",
    "            os.path.join(target_directory, source_names[int(passage_index)-1], \n",
    "                         targetname), 'w', encoding='utf-8') as f:\n",
    "            f.write(recall_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343175a8",
   "metadata": {},
   "source": [
    "Part 1 and Part 2 data were collected in slightly different contexts, so they are preprocessed a little differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa033ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each pt2 written recall file, extract text and remove boilerplate, \n",
    "# and save to correct location in `pooled`\n",
    "for path, subdirs, files in os.walk(\n",
    "    os.path.join(source_directory, 'recall', 'Written Recall Part 2')):\n",
    "    for name in files:\n",
    "        recall_path = str(pathlib.PurePath(path, name))\n",
    "        \n",
    "        # identify correct location in pooled\n",
    "        passage_index = recall_path[-7:-6]\n",
    "        subject_index, phase_index =  recall_path.split(name)[0][-3:-1], 3\n",
    "        if len(passage_index.strip()) == 0:\n",
    "            continue\n",
    "        targetname = '{}_{}_{}.txt'.format(\n",
    "            source_names[int(passage_index)-1], int(subject_index), phase_index)\n",
    "\n",
    "        # extract text and remove boilerplate\n",
    "        boilerplate = 'You have 5 minutes to type the story you just read for memory. There is no word limit. Please write as much as you can remember.'\n",
    "        recall_text = docxpy.process(\n",
    "            recall_path).replace(boilerplate, '').strip()\n",
    "        recall_text = '\\n'.join(recall_text.split('\\n')[1:]).strip()\n",
    "        \n",
    "        # clean text\n",
    "        recall_text = ftfy.fix_text(recall_text)\n",
    "        \n",
    "        # save to correct location\n",
    "        with open(os.path.join(\n",
    "            target_directory, source_names[int(passage_index)-1], \n",
    "            targetname), 'w', encoding='utf-8') as f:\n",
    "            f.write(recall_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e2969",
   "metadata": {},
   "source": [
    "### The result is an organized directory of text representations of participant responses absent methodology-specific details such as the content of the recall prompt."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
